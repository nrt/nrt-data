# Copyright (C) 2024 European Union
# 
# This program is free software: you can redistribute it and/or modify it under
# the terms of the EUROPEAN UNION PUBLIC LICENCE v. 1.2 as published by
# the European Union.
# 
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
# or FITNESS FOR A PARTICULAR PURPOSE.
# See the EUROPEAN UNION PUBLIC LICENCE v. 1.2 for further details.
# 
# You should have received a copy of the EUROPEAN UNION PUBLIC LICENCE v. 1.2.
# along with this program.
# If not, see <https://joinup.ec.europa.eu/collection/eupl/eupl-text-eupl-12 >

import datetime
from typing import Tuple, Optional

import numpy as np
import xarray as xr
from scipy.ndimage import gaussian_filter


def make_ts(dates, break_idx=-1, intercept=0.7, amplitude=0.15, magnitude=0.25,
            recovery_time=1095, sigma_noise=0.02, n_outlier=3,
            outlier_value=-0.1, n_nan=3):
    """Simulate a harmonic time-series with optional breakpoint, noise and outliers

    The time-series is generated by adding;
    - an intercept/trend component which varies depending on the phase of the time-series
    (stable, recovery)
    - An annual seasonal component
    - Random noise drawn from a normal distribution (white noise)
    Optional outliers are then added to randomly chosen observation as well as ``np.Nan`` values.
    Note that the seasonal cycles simulation approach used here is rather simplistic,
    using a sinusoidal model and therefore assuming symetrical and regular behaviour
    around the peak of the simulated variable. Actual vegetation signal is often more
    asymetrical and irregular.

    Args:
        dates (array-like): List or array of dates (numpy.datetime64)
        break_idx (int): Breakpoint index in the date array provided. Defaults to
            ``-1``, corresponding to a stable time-series
        intercept (float): Intercept of the time-series
        amplitude (float): Amplitude of the harmonic model (note that at every point
            of the time-series, the actual model amplitude is multiplied by the intercept
        magnitude (float): Break magnitude (always a drop in y value)
        recovery_time (int): Time (in days) to recover the initial intersect value
            following a break
        sigma_noise (float): Sigma value of the normal distribution (mean = 0) from which
            noise values are drawn
        n_outlier (int): Number of outliers randomly assigned to observations of the
            time-series
        outlier_value (float): Value to assign to outliers
        n_nan (int): Number of ``np.nan`` (no data) assigned to observations of the
            time-series

    Example:
        >>> from nrt.data import simulate
        >>> import numpy as np
        >>> import matplotlib.pyplot as plt

        >>> dates = np.arange('2018-01-01', '2022-06-15', dtype='datetime64[W]')
        >>> ts = simulate.make_ts(dates=dates, break_idx=30)

        >>> plt.plot(dates, ts)
        >>> plt.show()

    Returns:
        np.ndarray: Array of simulated values of same size as ``dates``
    """
    dates = dates.astype('datetime64[D]')
    timestamps = dates.astype(int)
    ydays = (dates - dates.astype('datetime64[Y]')).astype(int) + 1
    y = np.empty_like(dates, dtype=np.float64)
    # Intercept array
    y[:] = intercept
    # Build trend segment if break
    if break_idx != -1:
        # Segment bounds
        segment_start_y = intercept - magnitude
        segment_start_timestamp = timestamps[break_idx]
        segment_end_timestamp = segment_start_timestamp + recovery_time
        segment_end_idx = np.abs(segment_end_timestamp - timestamps).argmin()
        # Compute y values
        recovery_rate = magnitude / recovery_time
        days_since_break = timestamps - segment_start_timestamp
        trend_segment = (recovery_rate * days_since_break + segment_start_y)[break_idx + 1:segment_end_idx + 1]
        # include into y
        y[break_idx + 1:segment_end_idx + 1] = trend_segment
    # Seasonality
    amplitude_values = amplitude * y
    season = amplitude * np.sin(2 * np.pi * timestamps / 365.25 - 2)
    # noise and outliers
    noise = np.random.normal(0, sigma_noise, dates.size)
    # Combine the 3 (trend, season, noise) components
    ts = y + season + noise
    # Add optional outliers and Nans
    outliers_idx = np.random.choice(np.arange(0, dates.size), size=n_outlier, replace=False)
    nan_idx = np.random.choice(np.arange(0, dates.size), size=n_nan)
    ts[outliers_idx] = outlier_value
    ts[nan_idx] = np.nan
    return ts


def make_cube_parameters(shape=(100,100),
                         break_idx_interval=(0,100),
                         intercept_interval=(0.6, 0.8),
                         amplitude_interval=(0.12, 0.2),
                         magnitude_interval=(0.2, 0.3),
                         recovery_time_interval=(800,1400),
                         sigma_noise_interval=(0.02, 0.04),
                         n_outliers_interval=(0,5),
                         n_nan_interval=(0,5),
                         unstable_proportion=0.5):
    """Create ``xarray.Dataset`` of paramters for generation of synthetic data cube

    Prepares the main input required by the the ``make_cube`` function. This
    intermediary step eases the creation of multiple synthetic DataArrays sharing
    similar characteristics (e.g. to simulate multispectral data)

    Args:
        shape (tuple): A size two integer tuple giving the x,y size of the Dataset to be
            generated
        break_idx_interval (tuple): A tuple of two integers indicating the interval
            from which the breakpoint position in the time-series is drawn. Generate
            array of random values passed to the ``break_idx` argument of ``make_ts``.
            Similarly to python ranges, upper bound value is excluded from the resulting
            array. To produce a zero filled array ``(0,1)`` can therefore be used
            TODO: add a default to allow breakpoint at any location (conflict with Nan that indicate no break)
        intercept_interval (tuple): A tuple of two floats providing the interval
            from which intercept is drawn. Generate array of random values passed
            to the ``intercept`` argument of ``make_ts``
        amplitude_interval (tuple): A tuple of two floats indicating the interval
            from which the seasonal amplitude parameter is drawn. Generate array
            of random values passed to the ``amplitude`` argument of ``make_ts``
        magnitude_interval (tuple): A tuple of two floats indicating the interval
            from which the breakpoint magnitude parameter is drawn. Generate array
            of random values passed to the ``magnitude`` argument of ``make_ts``
        recovery_time_interval (tuple): A tuple of two integers indicating the interval
            from which the recovery time parameter (in days) is drawn. Generate array
            of random values passed to the ``recovery_time` argument of ``make_ts``
        sigma_noise_interval (tuple): A tuple of two floats indicating the interval
            from which the white noise level is drawn. Generate array of random
            values passed to the ``sigma_noise` argument of ``make_ts``
        n_outliers_interval (tuple): A tuple of two integers indicating the interval
            from which the number of outliers is drawn. Generate array
            of random values passed to the ``n_outliers` argument of ``make_ts``
        n_nan_interval (tuple): A tuple of two integers indicating the interval
            from which the number of no-data observations is drawn. Generate array
            of random values passed to the ``n_nan` argument of ``make_ts``
        unstable_proportion (float): Proportion of time-series containing a breakpoint.
            The other time-series are stable.

    Returns:
        xarray.Dataset: Dataset with arrays of parameters required for the generation
            of synthetic time-series using the spatialized version of ``make_ts``
            (see ``make_cube``)

    Examples:
        >>> import time
        >>> import numpy as np
        >>> import xarray as xr
        >>> from nrt.data import simulate
        >>> import matplotlib.pyplot as plt
        >>> params_nir = simulate.make_cube_parameters(shape=(20,20),
        ...                                            n_outliers_interval=(0,1),
        ...                                            n_nan_interval=(0,1),
        ...                                            break_idx_interval=(50,100))
        >>> params_red = params_nir.copy(deep=True)
        >>> # create parameters for red, green, blue cubes by slightly adjusting intercept,
        >>> # magnitude and amplitude parameters
        >>> params_red['intercept'].data = np.random.uniform(0.09, 0.12, size=(20,20))
        >>> params_red['magnitude'].data = np.random.uniform(-0.1, -0.03, size=(20,20))
        >>> params_red['amplitude'].data = np.random.uniform(0.03, 0.07, size=(20,20))
        >>> params_green = params_nir.copy(deep=True)
        >>> params_green['intercept'].data = np.random.uniform(0.12, 0.20, size=(20,20))
        >>> params_green['magnitude'].data = np.random.uniform(0.05, 0.1, size=(20,20))
        >>> params_green['amplitude'].data = np.random.uniform(0.05, 0.08, size=(20,20))
        >>> params_blue = params_nir.copy(deep=True)
        >>> params_blue['intercept'].data = np.random.uniform(0.08, 0.13, size=(20,20))
        >>> params_blue['magnitude'].data = np.random.uniform(-0.01, 0.01, size=(20,20))
        >>> params_blue['amplitude'].data = np.random.uniform(0.02, 0.04, size=(20,20))
        >>> dates = np.arange('2018-01-01', '2022-06-15', dtype='datetime64[W]')
        >>> # Create cubes (DataArrays) and merge them into a sligle Dataset
        >>> nir = data.make_cube(dates, name='nir', params_ds=params_nir)
        >>> red = data.make_cube(dates, name='red', params_ds=params_red)
        >>> green = data.make_cube(dates, name='green', params_ds=params_green)
        >>> blue = data.make_cube(dates, name='blue', params_ds=params_blue)
        >>> cube = xr.merge([blue, green, red, nir]).to_array()
        >>> # PLot one ts
        >>> cube.isel(x=5, y=5).plot(row='variable')
        >>> plt.show()
    """
    intercept = np.random.uniform(*intercept_interval, size=shape)
    amplitude = np.random.uniform(*amplitude_interval, size=shape)
    magnitude = np.random.uniform(*magnitude_interval, size=shape)
    recovery_time = np.random.randint(*recovery_time_interval, size=shape)
    sigma_noise = np.random.uniform(*sigma_noise_interval, size=shape)
    n_outlier = np.random.randint(*n_outliers_interval, size=shape)
    n_nan = np.random.randint(*n_nan_interval, size=shape)
    break_idx = np.random.randint(*break_idx_interval, size=shape)
    # Make a proportion of these cells stable
    size = np.multiply(*shape)
    stable_size = size - round(unstable_proportion * size)
    break_idx.ravel()[np.random.choice(size, stable_size, replace=False)] = -1
    # Build Dataset of parameters
    params = xr.Dataset(data_vars={'intercept': (['y', 'x'], intercept),
                                   'amplitude': (['y', 'x'], amplitude),
                                   'magnitude': (['y', 'x'], magnitude),
                                   'recovery_time': (['y', 'x'], recovery_time),
                                   'sigma_noise': (['y', 'x'], sigma_noise),
                                   'n_outlier': (['y', 'x'], n_outlier),
                                   'n_nan': (['y', 'x'], n_nan),
                                   'break_idx': (['y', 'x'], break_idx)},
                        coords={'y': np.arange(shape[0]),
                                'x': np.arange(shape[1])})
    return params


def make_cube(dates, params_ds, outlier_value=0.1, name='ndvi'):
    """Generate a cube of synthetic time-series

    See ``make_ts`` for more details on how every single time-series is generated

    Args:
        dates (array-like): List or array of dates (numpy.datetime64)
        params_ds (xarray.Dataset): Dataset containing arrays of time-series generation
            parameters. See ``make_cube_parameters`` for a helper to generate such Dataset.
            Spatial dimensions of the params_ds Dataset are used for the generated cube
        outlier_value (float): Value to assign to outliers
        name (str): Name of the generated variable in the DataArray

    Return:
        xarray.DataArray: Cube of synthetic time-series generated using the paramters
            provided via ``param_ds`` Dataset.

    Example:
        >>> import time
        >>> import numpy as np
        >>> from nrt.data import simulate
        >>> import matplotlib.pyplot as plt
        >>> dates = np.arange('2018-01-01', '2022-06-15', dtype='datetime64[W]')
        >>> params_ds = simulate.make_cube_parameters(shape=(100,100),
        ...                                           n_outliers_interval=(0,5),
        ...                                           n_nan_interval=(0,7),
        ...                                           break_idx_interval=(100,dates.size - 20))
        >>> cube = simulate.make_cube(dates=dates, params_ds=params_ds)
        >>> # PLot one ts
        >>> cube.isel(x=5, y=5).plot()
        >>> plt.show()
    """
    nrows, ncols = params_ds.intercept.data.shape
    # Vectorize function
    make_ts_v = np.vectorize(make_ts, signature='(n),(),(),(),(),(),(),(),(),()->(n)')
    # Create output array
    out = make_ts_v(dates=dates,
                    break_idx=params_ds.break_idx.data,
                    intercept=params_ds.intercept.data,
                    amplitude=params_ds.amplitude.data,
                    magnitude=params_ds.magnitude.data,
                    recovery_time=params_ds.recovery_time.data,
                    sigma_noise=params_ds.sigma_noise.data,
                    n_outlier=params_ds.n_outlier.data,
                    outlier_value=outlier_value,
                    n_nan=params_ds.n_nan.data)
    # Build xarray dataset
    xr_cube = xr.DataArray(data=np.moveaxis(out, -1, 0),
                           coords={'time': dates,
                                   'y': np.arange(nrows),
                                   'x': np.arange(ncols)},
                          name=name)
    return xr_cube


def make_landscape(
    shape: Tuple[int, int] = (5000, 5000),
    year: int = 2020,
    forest_pct: float = 0.70,
    loss_pct: float = 0.03,
    forest_compactness: float = 60.0,
    disturbance_clustering: float = 30.0,
    disturbance_roughness: float = 3.0,
    disturbance_roughness_share: float = 0.1,
    seed: Optional[int] = None
) -> Tuple[np.ndarray, np.ndarray]:
    """Generates a synthetic landscape with spatio-temporally correlated forest loss.

    Uses Gaussian filtering of white noise to create spatially autocorrelated
    structures for both forest cover and disturbance patches.
    Temporal dates are assigned using a gradient from patch center (early) to
    edge (late).

    Args:
        shape (tuple): Dimensions of the output array (rows, cols). Defaults to (5000, 5000).
        year (int): The year to simulate. Dates will be returned as days since
            1970-01-01, starting from Jan 1st of this year. Defaults to 2020.
        forest_pct (float): Target percentage of forest cover (0.0 to 1.0).
            Defaults to 0.70.
        loss_pct (float): Target percentage of forest loss relative to the total
            landscape area. Defaults to 0.03.
        forest_compactness (float): Sigma for forest smoothing. Higher values create
            larger, smoother contiguous forest blocks. Defaults to 60.0.
        disturbance_clustering (float): Sigma for disturbance structure. Higher values
            create larger, fewer disturbance patches. Defaults to 30.0.
        disturbance_roughness (float): Sigma for disturbance texture. Lower values
            create jagged, scattered pixel edges. Defaults to 3.0.
        disturbance_roughness_share (float): Weight (0.0 to 1.0) of roughness vs
            clustering. Higher values add more salt-and-pepper noise.
            Defaults to 0.1.
        seed: Random seed for reproducibility. Defaults to None.

    Examples:
        >>> import numpy as np
        >>> from nrt.data import simulate
        >>> from matplotlib import pyplot as plt

        >>> mask, disturbance = simulate.make_landscape(shape=(2000,2000),
        ...                                             forest_pct=0.60,
        ...                                             loss_pct=0.02,
        ...                                             seed=42)

        >>> # For mask plot make green forests and magenta disturbances
        >>> # For disturbance plot use a colormap that goes from jan 2020 to dec 2020 (in days since 1970)
        >>> fig, ax = plt.subplots(1, 2, figsize=(16, 8))
        >>> # 1. Land Cover Map
        >>> cmap_lc = plt.cm.colors.ListedColormap(['#eecfa1', '#228b22', '#ff00ff'])
        >>> ax[0].imshow(mask, cmap=cmap_lc, interpolation='none')
        >>> ax[0].set_title("Mask")
        >>> ax[0].axis('off')
        >>> # 2. Date of Disturbance Map
        >>> masked_disturbance = np.ma.masked_where(disturbance == 0, disturbance)
        >>> im = ax[1].imshow(masked_disturbance, cmap='jet_r', interpolation='none')
        >>> ax[1].set_title("Disturbance date (Temporally Correlated)")
        >>> ax[1].axis('off')
        >>> plt.colorbar(im, ax=ax[1], label="Day since 1970", fraction=0.046, pad=0.04)
        >>> plt.tight_layout()
        >>> plt.show()

    Returns:
        A tuple (land_cover, loss_dates) where:

        * **land_cover** (np.ndarray): int8 array where 0=Non-Forest,
          1=Forest, 2=Loss.
        * **loss_dates** (np.ndarray): int32 array representing the date of
          loss as days since 1970-01-01. Pixels with no loss are 0.
    """
    if seed is not None:
        np.random.seed(seed)
    # 1. Generate Forest Cover
    # High sigma creates large contiguous 'continents'
    noise_forest = np.random.rand(*shape)
    smooth_forest = gaussian_filter(noise_forest, sigma=forest_compactness)
    forest_threshold = np.percentile(smooth_forest, (1 - forest_pct) * 100)
    is_forest = smooth_forest >= forest_threshold

    # 2. Generate Disturbance Potential
    # Mix large blobs (structure) with fine grain noise (roughness/texture)
    noise_struct = gaussian_filter(np.random.rand(*shape), sigma=disturbance_clustering)
    noise_text = gaussian_filter(np.random.rand(*shape), sigma=disturbance_roughness)

    # Weighted composition
    w_struct = 1.0 - disturbance_roughness_share
    loss_potential = (w_struct * noise_struct) + (disturbance_roughness_share * noise_text)

    # 3. Apply Loss Threshold (Strictly inside Forest)
    valid_potential = loss_potential[is_forest]
    # Calculate how many pixels we need relative to the forest area
    target_px = shape[0] * shape[1] * loss_pct

    # Safety check for 0 loss
    if target_px > 0 and len(valid_potential) > 0:
        relative_pct = target_px / len(valid_potential)
        loss_cutoff = np.percentile(valid_potential, (1 - relative_pct) * 100)
        is_loss = (loss_potential >= loss_cutoff) & is_forest
    else:
        is_loss = np.zeros(shape, dtype=bool)

    # 4. Temporal Attribution (Gradient Method)
    # Use int32 for absolute dates (days since 1970 can exceed int16 limit)
    loss_dates = np.zeros(shape, dtype=np.int32)

    if np.any(is_loss):
        # Calculate start offset (Days from 1970-01-01 to Year-01-01)
        start_date = datetime.date(year, 1, 1)
        epoch = datetime.date(1970, 1, 1)
        start_offset = (start_date - epoch).days

        loss_values = loss_potential[is_loss]
        v_min, v_max = loss_values.min(), loss_values.max()

        # Normalize 0..1
        norm_values = (loss_values - v_min) / (v_max - v_min)

        # Invert: Center (High Pot) -> Day 1, Edge (Low Pot) -> Day 365
        # We assume loss spans the full year (1-365)
        doy_values = 1 + ((1 - norm_values) * 364).astype(np.int32)

        # Add jitter
        jitter = np.random.randint(-3, 4, size=doy_values.shape)
        doy_values = np.clip(doy_values + jitter, 1, 365)

        # Convert to Days since 1970
        # date = start_offset + doy - 1
        loss_dates[is_loss] = start_offset + doy_values - 1

    # 5. Assemble Land Cover
    land_cover = np.zeros(shape, dtype=np.int8)
    land_cover[is_forest] = 1
    land_cover[is_loss] = 2  # Overwrite forest with loss

    return land_cover, loss_dates


def make_prediction(
    ref_lc: np.ndarray,
    ref_dates: np.ndarray,
    omission_rate: float = 0.10,
    commission_rate: float = 0.01,
    lag_mean: float = 15.0,
    lag_std: float = 10.0,
    seed: Optional[int] = None
) -> Tuple[np.ndarray, np.ndarray]:
    """Simulates a monitoring algorithm's output by degrading reference data.

    Adds omission errors, spatially correlated commission errors, and temporal
    detection lag. Detection lag is modeled using a Gamma distribution to
    simulate the 'long tail' of delayed detections often seen in satellite
    alerts.
    This function is ideally used in combination with ``inrt.data.simulate.make_landscape``.

    Args:
        ref_lc (np.ndarray): Reference land cover array (0=Non, 1=Forest, 2=Loss).
        ref_dates (np.ndarray): Reference dates of loss (days since 1970).
        omission_rate (float): Probability (0.0 to 1.0) of missing a true loss pixel.
            Defaults to 0.10.
        commission_rate (float): Probability (0.0 to 1.0) of falsely flagging a
            stable pixel as loss. Applied to the Total Stable Area.
            Defaults to 0.01.
        lag_mean (float): Mean delay in days between event and detection.
            Defaults to 15.0.
        lag_std (float): Standard deviation of the delay in days. Defaults to 10.0.
        seed (int): Random seed for reproducibility. Defaults to None.

    Examples:
        >>> import numpy as np
        >>> import matplotlib.pyplot as plt
        >>> from nrt.data import simulate

        >>> # 1. Generate Reference and Prediction
        >>> mask, dates = make_landscape(shape=(1000, 1000), seed=42)
        >>> pred_mask, pred_dates = make_prediction(mask, dates, seed=42)

        >>> # 2. Compute Accuracy
        >>> # Find ALL spatial matches (Intersection of Reference and Prediction)
        >>> spatial_match = (mask == 2) & (pred_mask == 1)
        >>>
        >>> # Calculate lags for all matches
        >>> all_lags = pred_dates[spatial_match].astype(float) - dates[spatial_match].astype(float)
        >>>
        >>> # Apply Temporal Rules: Valid if lag is within [-5, +30] days
        >>> valid_window = (all_lags >= -5) & (all_lags <= 30)
        >>> tp_count = np.count_nonzero(valid_window)
        >>>
        >>> ua = tp_count / np.count_nonzero(pred_mask == 1)
        >>> pa = tp_count / np.count_nonzero(mask == 2)
        >>> print(f"UA: {ua:.1%}, PA: {pa:.1%}")
        UA: 74.0%, PA: 83.4%

        >>> # 3. Visualize Lag Distribution (All Matches)
        >>> plt.figure(figsize=(7, 4))
        >>> # Plot all matches in grey (background)
        >>> plt.hist(all_lags, bins=range(-20, 60), color='lightgrey', label='Rejected Matches')
        >>> # Overlay True Positives in teal
        >>> plt.hist(all_lags[valid_window], bins=range(-20, 60), color='teal', label='True Positives')
        >>>
        >>> # Add Tolerance Lines
        >>> plt.axvline(x=-5, color='red', linestyle='--', linewidth=1, label='Tolerance (-5, +30)')
        >>> plt.axvline(x=30, color='red', linestyle='--', linewidth=1)
        >>>
        >>> plt.xlabel('Detection Lag (days)')
        >>> plt.ylabel('Pixel Count')
        >>> plt.title(f'Temporal Accuracy Analysis (N={len(all_lags)})')
        >>> plt.legend()
        >>> plt.show()

    Returns:
        A tuple (pred_lc, pred_dates) where:

        * **pred_lc** (np.ndarray): int8 array where 0=Stable, 1=Loss.
        * **pred_dates** (np.ndarray): int32 array representing detection
          dates as days since 1970-01-01.
    """
    if seed is not None:
        np.random.seed(seed)
    shape = ref_lc.shape
    # --- 1. Handle True Positives (TP) ---
    # TP must be Class 2 (Loss) AND have a valid date (>0)
    true_loss_mask = (ref_lc == 2) & (ref_dates > 0)
    omission_mask = np.random.random(shape) < omission_rate
    detected_tp_mask = true_loss_mask & (~omission_mask)

    # --- 2. Handle False Positives (FP) ---
    # RESTRICTION: Commission errors only allowed in STABLE FOREST (Class 1)
    # Class 0 (Non-Forest) is excluded.
    stable_forest_mask = (ref_lc == 1)
    noise_fp = gaussian_filter(np.random.rand(*shape), sigma=10)
    valid_fp_noise = noise_fp[stable_forest_mask]

    if len(valid_fp_noise) > 0:
        # Rate applies to the Stable Forest area
        fp_cutoff = np.percentile(valid_fp_noise, (1 - commission_rate) * 100)
        detected_fp_mask = (noise_fp >= fp_cutoff) & stable_forest_mask
    else:
        detected_fp_mask = np.zeros(shape, dtype=bool)

    # --- 3. Assemble Output ---
    pred_lc = np.zeros(shape, dtype=np.int8)
    pred_lc[detected_tp_mask | detected_fp_mask] = 1
    pred_dates = np.zeros(shape, dtype=np.int32)

    # A. Dates for TPs (Reference + Lag)
    if np.any(detected_tp_mask):
        real_dates = ref_dates[detected_tp_mask]
        theta = (lag_std ** 2) / lag_mean
        k = lag_mean / theta
        lags = np.random.gamma(k, theta, size=np.count_nonzero(detected_tp_mask))
        lags = np.round(lags).astype(np.int32)
        pred_dates[detected_tp_mask] = real_dates + lags

    # B. Dates for FPs (Random within context)
    if np.any(detected_fp_mask):
        # Infer context year from reference data
        if np.any(ref_dates > 0):
            min_d, max_d = ref_dates[ref_dates > 0].min(), ref_dates[ref_dates > 0].max()
        else:
            min_d, max_d = 18262, 18627 # 2020 fallback

        fp_dates = np.random.randint(min_d, max_d + 1, size=np.count_nonzero(detected_fp_mask))
        pred_dates[detected_fp_mask] = fp_dates

    return pred_lc, pred_dates


if __name__ == "__main__":
    import doctest
    doctest.testmod()

